{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = \"C:/Users/Oscar/Desktop/jd_xiaomi9_pos.csv\"\n",
    "neg_path = \"C:/Users/Oscar/Desktop/jd_xiaomi9_neg.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file = open(pos_path)\n",
    "neg_file = open(neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reader_lines = csv.reader(pos_file)\n",
    "neg_reader_lines = csv.reader(neg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "train_texts_orig = []\n",
    "# 文本所对应的labels, 也就是标记\n",
    "train_target = []\n",
    "for line in pos_reader_lines:\n",
    "    train_texts_orig.append(line[1])\n",
    "    train_target.append(line[2])\n",
    "for line in neg_reader_lines:\n",
    "    train_texts_orig.append(line[1])\n",
    "    train_target.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 用来解压\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请将下载的词向量压缩包放置在根目录 embeddings 文件夹里\n",
    "# 解压词向量, 有可能需要等待1-2分钟\n",
    "with open(\"dataset/NLP/sgns.zhihu.bigram\", 'wb') as new_file, open(\"dataset/NLP/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('dataset/NLP/sgns.zhihu.bigram', binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.023750e-01, -3.708000e-03, -7.565430e-01,  8.173280e-01,\n",
       "       -1.443002e+00, -1.169990e-01,  3.027310e-01,  7.402350e-01,\n",
       "        1.360870e-01,  1.137900e-01, -4.726100e-01,  6.012690e-01,\n",
       "       -4.950800e-02,  1.690520e-01,  4.720550e-01, -1.202050e-01,\n",
       "       -2.324800e-02, -7.101900e-02, -4.373060e-01,  5.065900e-02,\n",
       "        4.102140e-01, -6.221000e-03, -1.446990e-01,  6.171340e-01,\n",
       "       -1.099430e-01, -5.144020e-01, -2.406990e-01, -5.681100e-01,\n",
       "       -5.727620e-01,  1.198359e+00, -1.361800e-02,  1.860260e-01,\n",
       "       -2.040300e-02,  5.441100e-02, -1.790730e-01, -2.648950e-01,\n",
       "       -5.086800e-02, -3.325420e-01, -3.877390e-01,  1.861580e-01,\n",
       "       -8.618210e-01,  4.052920e-01,  3.045350e-01, -8.005700e-02,\n",
       "        4.662680e-01,  1.387900e-01, -4.225740e-01, -3.337170e-01,\n",
       "       -9.828600e-02,  5.087270e-01, -1.998500e-02,  3.137330e-01,\n",
       "        2.240040e-01,  1.746050e-01,  1.361020e-01,  9.837300e-02,\n",
       "       -1.522923e+00,  7.467910e-01,  1.645960e-01,  6.225840e-01,\n",
       "       -1.604020e-01,  5.917870e-01, -9.480880e-01, -1.435300e-02,\n",
       "        4.626270e-01, -3.225100e-02,  2.440550e-01,  5.190190e-01,\n",
       "        2.778970e-01, -4.285780e-01, -6.062920e-01,  3.720070e-01,\n",
       "       -3.529080e-01, -1.166835e+00, -5.558340e-01,  6.828600e-02,\n",
       "       -2.996020e-01,  6.206000e-02,  3.960730e-01,  8.334040e-01,\n",
       "        1.044498e+00,  1.337680e-01, -2.029630e-01,  2.196920e-01,\n",
       "       -1.781260e-01, -3.014300e-02,  6.935000e-02,  6.249800e-02,\n",
       "        2.886550e-01, -4.502850e-01, -2.477160e-01,  4.880530e-01,\n",
       "        3.698890e-01,  3.338210e-01, -1.557300e-02,  9.900210e-01,\n",
       "       -1.707440e-01, -3.829450e-01,  4.459190e-01,  1.236000e-01,\n",
       "        3.201700e-01,  8.550600e-01, -2.337900e-01,  3.132880e-01,\n",
       "        3.929200e-02, -2.351200e-01, -5.951000e-02, -6.581600e-02,\n",
       "        2.919000e-01, -1.583100e-02, -3.136600e-01,  7.610300e-01,\n",
       "       -6.344710e-01,  4.327400e-02,  4.469480e-01, -3.408500e-02,\n",
       "        1.082450e-01, -2.311600e-02,  2.521700e-02, -5.389210e-01,\n",
       "        2.847920e-01,  4.541150e-01,  4.113190e-01, -6.318450e-01,\n",
       "       -4.152820e-01, -4.059990e-01, -1.841030e-01, -2.417000e-02,\n",
       "       -4.433330e-01, -2.915200e-01,  5.356230e-01, -3.234210e-01,\n",
       "       -2.768630e-01,  4.274750e-01,  5.194200e-02,  4.308780e-01,\n",
       "       -1.981320e-01,  3.385260e-01,  7.712300e-02, -1.404390e-01,\n",
       "       -1.022367e+00,  8.251610e-01,  9.024140e-01, -9.871870e-01,\n",
       "       -1.449370e-01, -3.818000e-03,  5.912800e-02,  4.830890e-01,\n",
       "       -1.881900e-01,  5.773890e-01, -2.537190e-01, -5.265690e-01,\n",
       "       -7.126080e-01,  1.128180e-01,  2.260650e-01, -6.787400e-02,\n",
       "       -4.136700e-01, -1.972380e-01, -1.014900e-01,  4.247090e-01,\n",
       "        4.957900e-02,  1.656430e-01, -9.298840e-01, -6.911000e-02,\n",
       "        7.162640e-01, -1.132230e-01, -6.371870e-01, -3.256160e-01,\n",
       "        8.002400e-01,  1.408300e-02, -3.321370e-01, -1.727000e-03,\n",
       "       -3.210130e-01,  3.629850e-01,  3.243720e-01, -5.371790e-01,\n",
       "       -2.071700e-02,  5.962380e-01, -6.609010e-01,  1.621130e-01,\n",
       "       -3.205570e-01, -2.290100e-02,  4.662390e-01,  4.586100e-01,\n",
       "       -3.627000e-02, -6.346520e-01,  3.939420e-01, -2.071540e-01,\n",
       "        5.147880e-01,  1.620720e-01, -2.550190e-01,  5.449500e-02,\n",
       "       -4.144870e-01,  1.068840e-01,  6.159670e-01, -4.086540e-01,\n",
       "        7.772840e-01, -1.130070e-01, -3.068530e-01,  4.983890e-01,\n",
       "       -2.936340e-01, -1.790040e-01, -1.696860e-01, -4.881490e-01,\n",
       "        1.390250e-01,  2.039220e-01, -5.919880e-01,  3.772360e-01,\n",
       "       -3.034450e-01, -4.576750e-01, -4.386610e-01,  6.900150e-01,\n",
       "       -1.296840e-01,  8.518600e-02,  7.360600e-02, -1.918000e-01,\n",
       "        4.311550e-01,  9.799600e-01, -5.693600e-01,  3.638110e-01,\n",
       "        9.039320e-01,  6.212190e-01,  1.735400e-01, -6.542760e-01,\n",
       "       -1.526040e-01, -3.133230e-01,  7.057380e-01, -1.314080e-01,\n",
       "       -2.305210e-01, -9.629230e-01, -1.655250e-01,  2.117000e-03,\n",
       "        2.418030e-01, -1.705200e-02, -5.440710e-01, -3.727720e-01,\n",
       "       -2.254140e-01,  1.069380e+00,  2.201900e-01,  4.876930e-01,\n",
       "       -2.369860e-01,  2.445330e-01,  8.051320e-01,  1.275750e-01,\n",
       "       -1.203510e-01, -3.310060e-01,  4.421000e-03, -1.262900e-01,\n",
       "        5.256590e-01, -2.576570e-01,  1.380559e+00, -3.204940e-01,\n",
       "       -2.930040e-01, -1.189100e-01,  5.338550e-01, -7.152440e-01,\n",
       "       -6.210090e-01, -9.269320e-01, -4.313620e-01, -6.477290e-01,\n",
       "       -1.530820e-01, -5.256420e-01, -6.579390e-01, -2.754140e-01,\n",
       "       -1.704300e-02,  2.403390e-01, -3.873110e-01, -6.459600e-02,\n",
       "       -3.706200e-02,  1.062040e-01, -5.316800e-01, -1.205610e-01,\n",
       "        1.911560e-01,  4.095740e-01,  8.015600e-02, -1.581500e-01,\n",
       "       -1.534810e-01, -1.546410e-01,  1.101710e-01,  1.592950e-01,\n",
       "       -6.530000e-04, -2.529560e-01,  6.500510e-01, -6.791290e-01,\n",
       "        8.227590e-01,  7.281170e-01, -2.114910e-01,  3.008630e-01,\n",
       "        7.394000e-03, -3.509600e-02, -5.096690e-01, -3.428370e-01,\n",
       "       -5.468130e-01,  7.764400e-02, -5.807150e-01, -8.612000e-02,\n",
       "        4.752600e-02, -1.162530e-01, -2.488900e-02, -3.586300e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['深圳'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "cn_model['深圳']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77130115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.similarity('酒店','宾馆')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('涮羊肉', 0.7356440424919128),\n",
       " ('烤鱼', 0.7211310267448425),\n",
       " ('麻辣锅', 0.7164487838745117),\n",
       " ('烤全羊', 0.7161980867385864),\n",
       " ('炒年糕', 0.7136646509170532),\n",
       " ('咖喱饭', 0.7114191651344299),\n",
       " ('羊排', 0.7113378047943115),\n",
       " ('干锅', 0.7113052606582642),\n",
       " ('川味', 0.7098793387413025),\n",
       " ('麻辣火锅', 0.7087246179580688)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['烤肉'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这个花屏是什么鬼啊。拿到第二天'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[1700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Oscar\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.751 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(token) for token in train_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.716751269035534"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9527918781725888"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为98时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'物流挺好手机各种问题都是共性问题都别买了'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 60000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,  4738,  1851,     3,    71,     3,    72,     0,     3,\n",
       "         470,    72,     1,  7859,    34,    72, 11056,  1304,  2350,\n",
       "       14441,   227,     0,    71, 16068, 14441, 11056,     0,     3,\n",
       "         470,    72,     1,   300,    34, 18151,    34,   772,  1575,\n",
       "        8046,  2657, 51127, 55422,     0, 14594,   401,  4858,   262,\n",
       "           1,   300,     4,  5378,     0,     1,    36,  4376,  1007,\n",
       "         348,    95,   223,  3465,  1304,    82,  2811,    98,     0,\n",
       "         616,     7,   371,   515,   470,    72,     1,   518])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_target.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "充电快速度快感觉就像送外卖我很喜欢米酒功能好我很喜欢玩游戏一点都 拍照功能也差不多很好太喜欢了喜欢的朋友可以考虑一下哦我真的不是吹的功能太好了玩游戏一点都 指纹开锁也挺快的音质也很好小爱同学也很听话充电充一个小时就充满了特别的快买的时候就是上午下午就到了说了那么多因为我不是托感谢大家来观看希望 个赞 \n",
      "class:  1\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[100]))\n",
    "print('class: ',y_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                   embedding_dim,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length = max_tokens,\n",
    "                   trainable = False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 98, 300)           18000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 98, 128)           186880    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 18,196,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 18,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'dataset/NLP/sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-8, patience=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1595 samples, validate on 178 samples\n",
      "Epoch 1/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.1494 - acc: 0.9564\n",
      "Epoch 00001: val_loss improved from inf to 0.11786, saving model to dataset/NLP/sentiment_checkpoint.keras\n",
      "1595/1595 [==============================] - 7s 5ms/step - loss: 0.1473 - acc: 0.9574 - val_loss: 0.1179 - val_acc: 0.9663\n",
      "Epoch 2/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.1231 - acc: 0.9668\n",
      "Epoch 00002: val_loss did not improve from 0.11786\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.1216 - acc: 0.9674 - val_loss: 0.1181 - val_acc: 0.9663\n",
      "Epoch 3/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.0917 - acc: 0.9779\n",
      "Epoch 00003: val_loss improved from 0.11786 to 0.11692, saving model to dataset/NLP/sentiment_checkpoint.keras\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.0920 - acc: 0.9781 - val_loss: 0.1169 - val_acc: 0.9663\n",
      "Epoch 4/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.0886 - acc: 0.9792\n",
      "Epoch 00004: val_loss did not improve from 0.11692\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.0873 - acc: 0.9799 - val_loss: 0.1344 - val_acc: 0.9494\n",
      "Epoch 5/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.0851 - acc: 0.9772\n",
      "Epoch 00005: val_loss did not improve from 0.11692\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.0836 - acc: 0.9781 - val_loss: 0.1365 - val_acc: 0.9494\n",
      "Epoch 6/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.0840 - acc: 0.9785\n",
      "Epoch 00006: val_loss did not improve from 0.11692\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.0834 - acc: 0.9787 - val_loss: 0.1366 - val_acc: 0.9494\n",
      "Epoch 7/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.0839 - acc: 0.9785\n",
      "Epoch 00007: val_loss did not improve from 0.11692\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.0834 - acc: 0.9787 - val_loss: 0.1366 - val_acc: 0.9494\n",
      "Epoch 8/20\n",
      "1536/1595 [===========================>..] - ETA: 0s - loss: 0.0847 - acc: 0.9785\n",
      "Epoch 00008: val_loss did not improve from 0.11692\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "1595/1595 [==============================] - 5s 3ms/step - loss: 0.0834 - acc: 0.9787 - val_loss: 0.1366 - val_acc: 0.9494\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22780b11d30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=256,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 0s 2ms/step\n",
      "Accuracy:96.95%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 30000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "品控不好，还没到一个月就坏了\n",
      "是一例负面评价 output=0.03\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment('品控不好，还没到一个月就坏了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          GPS无效不能导航 硬伤手机发烫这是小米通病电池耗电较快其他还行京东自营快递还是 \n",
      "预测的分类 0\n",
      "实际的分类 0\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,  21,  74,  87, 183, 188], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手机用起来很舒服，外观造型漂亮，很流畅，屏幕很清晰\n",
      "是一例正面评价 output=0.96\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment('手机用起来很舒服，外观造型漂亮，很流畅，屏幕很清晰')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
